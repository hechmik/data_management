{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "import json\n",
    "from sshtunnel import SSHTunnelForwarder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook there are the code chunks used for downloading the data stored in the VM collection and the ones useful for inserting the data in another instance. The recommended way for exporting and importing data on MongoDB is through the \"mongoimport\" and \"mongoexport\" utilities. We have chosen to use a programmatic approach because it helps us to filter the data we'd like to export: in particular we just wanted to export data stored before the 28th of June because that was the week when we wrote the project report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download data from VM collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SSH Tunnel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../keys.json') as k:\n",
    "    keys = json.loads(k.read())\n",
    "    MONGO_HOST = keys[\"SSH\"][\"MONGO_HOST\"] # i.e. 10.9.13.14\n",
    "    MONGO_DB = keys[\"SSH\"][\"MONGO_DB\"] # i.e. dm_project \n",
    "    MONGO_USER = keys[\"SSH\"][\"MONGO_USER\"] # i.e. studente\n",
    "    MONGO_PASS = keys[\"SSH\"][\"MONGO_PASS\"] # i.e. la password della vm\n",
    "\n",
    "server = SSHTunnelForwarder(\n",
    "    MONGO_HOST,\n",
    "    ssh_username=MONGO_USER,\n",
    "    ssh_password=MONGO_PASS,\n",
    "    remote_bind_address=('127.0.0.1', 27017)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "server.start() # remember to stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = pymongo.MongoClient('127.0.0.1', server.local_bind_port) # server.local_bind_port is assigned local port\n",
    "db = client[MONGO_DB]\n",
    "db.list_collection_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The import of the dumps function is useful for converting ObjectID and datetime structures in JSON-Compatible ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bson.json_util import dumps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export twitch data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Twitch collection is by far the biggest one on our project. For this reason we will work on smaller subsets, so that it will be possible to download the collection files without incurring in Memory Errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = [{ \"$match\" : {'timestamp' : {'$lt' : '2019-06-15'}}}]\n",
    "twitch_cursor = db.twitch.aggregate(pipeline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitch_dump_before_06_15 = dumps(twitch_cursor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"twitch_before_06_15.json\", \"w\") as f:\n",
    "        f.write(twitch_dump_before_06_15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_15_20 = [{ \"$match\" : {'$and' : [\n",
    "                {'timestamp' : {'$gte' : '2019-06-15'}},\n",
    "                {'timestamp' : {'$lt' : '2019-06-20'}}\n",
    "            ]}}]\n",
    "twitch_cursor_15_20 = db.twitch.aggregate(pipeline_15_20)\n",
    "twitch_dump_15_20 = dumps(twitch_cursor_15_20)\n",
    "with open(\"twitch_06_15_20.json\", \"w\") as f:\n",
    "        f.write(twitch_dump_15_20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_20_24 = [{ \"$match\" : {'$and' : [\n",
    "                {'timestamp' : {'$gte' : '2019-06-20'}},\n",
    "                {'timestamp' : {'$lt' : '2019-06-24'}}\n",
    "            ]}}]\n",
    "twitch_cursor_20_24 = db.twitch.aggregate(pipeline_20_24)\n",
    "twitch_dump_20_24 = dumps(twitch_cursor_20_24)\n",
    "with open(\"twitch_06_20_24.json\", \"w\") as f:\n",
    "        f.write(twitch_dump_20_24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del twitch_dump_20_24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_24_28 = [{ \"$match\" : {'$and' : [\n",
    "                {'timestamp' : {'$gte' : '2019-06-24'}},\n",
    "                {'timestamp' : {'$lt' : '2019-06-28'}}\n",
    "            ]}}]\n",
    "twitch_cursor_24_28 = db.twitch.aggregate(pipeline_24_28)\n",
    "twitch_dump_24_28 = dumps(twitch_cursor_24_28)\n",
    "with open(\"twitch_06_24_28.json\", \"w\") as f:\n",
    "        f.write(twitch_dump_24_28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del twitch_dump_24_28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export twitter collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.twitter.find_one()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "end = datetime(2019, 6, 28, 0, 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = [{ \"$match\" : {'date' : {'$lt' : end}}}]\n",
    "twitter = db.twitter.aggregate(pipeline)\n",
    "twitter_dump = dumps(twitter)\n",
    "with open(\"twitter.json\", \"w\") as f:\n",
    "        f.write(twitter_dump)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rember to close the connection!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "server.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insert download data on another MongoDB Instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important: change all IP and path according to your sistem configuration. Do not change the DB and collections names, otherwise the other notebooks won't work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = pymongo.MongoClient(\"localhost\", 27017)\n",
    "db = client[\"dm_project\"]\n",
    "db.list_collection_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bson.json_util import loads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json_dump(fn):\n",
    "    with open(fn) as f:\n",
    "        collection = loads(f.read())\n",
    "    return collection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitch_files = [\"twitch_before_06_15.json\", \"twitch_06_15_20.json\", \"twitch_06_20_24.json\", \"twitch_06_24_28.json\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in twitch_files:\n",
    "    print(file)\n",
    "    collection = load_json_dump(file)\n",
    "    print(\"Loaded\")\n",
    "    db.twitch.insert_many(collection)\n",
    "    print(\"Inserted!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if data was loaded correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.twitch.find_one()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.twitch.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insert Twitter Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_collection = load_json_dump(\"twitter.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.twitter.insert_many(twitter_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.twitter.find_one()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
